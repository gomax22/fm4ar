data:
  # DATA SETTINGS

# Model architecture
model:
  type: nsf+embedding
  # kwargs for neural spline flow
  nsf_kwargs:
    num_flow_steps: 30
    base_transform_kwargs:
      hidden_dim: 1024
      num_transform_blocks: 5
      activation: elu
      dropout_probability: 0.0
      batch_norm: True
      num_bins: 8
      base_transform_type: rq-coupling
  # kwargs for embedding net
  embedding_net_kwargs:
    output_dim: 128
    hidden_dims: [1024, 1024, 1024, 1024, 1024, 1024,
                  512, 512, 512, 512, 512, 512,
                  256, 256, 256, 256, 256, 256,
                  128, 128, 128, 128, 128, 128]
    activation: elu
    dropout: 0.0
    batch_norm: True

# Training is divided in stages. They each require all settings as indicated below.
training:
  stage_0:
    epochs: 300
    optimizer:
      type: adam
      lr: 5.0e-5
    scheduler:
      type: cosine
      T_max: 300
    batch_size: 4096

  stage_1:
    epochs: 150
    optimizer:
      type: adam
      lr: 1.0e-5
    scheduler:
      type: cosine
      T_max: 150
    batch_size: 4096

# Local settings for training that have no impact on the final trained network.
local:
  device: cuda # Change this to 'cuda' for training on a GPU.
  num_workers: 32  # num_workers >0 does not work on Mac, see https://stackoverflow.com/questions/64772335/pytorch-w-parallelnative-cpp206
  runtime_limits:
    max_time_per_run: 36000
    max_epochs_per_run: 500
  checkpoint_epochs: 50
  # Local settings related to condor, remove if not used on cluster
  condor:
    bid: 100
    num_cpus: 16
    memory_cpus: 128000
    num_gpus: 1
    memory_gpus: 8000