# -----------------------------------------------------------------------------
# DATASET
# -----------------------------------------------------------------------------

data:
  name: vasist-2023
  which: training
  train_fraction: 0.95

# -----------------------------------------------------------------------------
# MODEL ARCHITECTURE
# -----------------------------------------------------------------------------

model:

  # General settings
  model_type: fm
  t_theta_with_glu: True
  context_with_glu: False
  sigma_min: 0.0001
  time_prior_exponent: 0.5

  # Embedding network for context
  context_embedding_kwargs:
    01_SoftClip:
      model_type: SoftClip
      kwargs:
        bound: 10.0
    02_DenseResidualNet:
      model_type: DenseResidualNet
      kwargs:
        hidden_dims: [512, 512, 512, 512, 512, 256, 256, 128, 128]
        activation: elu
        output_dim: 128
        dropout: 0.0
        batch_norm: False

  # Embedding network for t_theta
  t_theta_embedding_kwargs:
    01_PositionalEncoding:
      model_type: PositionalEncoding
      kwargs:
        n_freqs: 5
        encode_theta: False
    02_DenseResidualNet:
      model_type: DenseResidualNet
      kwargs:
        hidden_dims: [256, 128, 64, 32]
        activation: gelu
        output_dim: 32
        dropout: 0.0
        batch_norm: False

  # Posterior network (i.e., the network that predicts the vector field)
  posterior_kwargs:
    model_type: DenseResidualNet
    kwargs:
      hidden_dims: [
        2048,
        2048,
        1024,
        1024,
        512,
        512,
        512,
        256,
        256,
        256,
        128,
        128,
        64,
        64,
        32,
        32,
        16,
        16,
      ]
      activation: gelu
      dropout: 0.3
      batch_norm: False


# -----------------------------------------------------------------------------
# TRAINING
# -----------------------------------------------------------------------------

training:
  stage_0:
    epochs: 300
    optimizer:
      type: adamw
      lr: 5.0e-4
    scheduler:
      type: reduce_on_plateau
      factor: 0.5
      patience: 10
    batch_size: 4096
    early_stopping:
      patience: 100
      min_delta: 0.0

# -----------------------------------------------------------------------------
# LOCAL SETTINGS
# -----------------------------------------------------------------------------

local:

  # General settings: device, runtime limits, ...
  # Note: If training locally on a Mac, set num_workers to 0!
  device: cuda
  num_workers: 4
  runtime_limits:
    max_time_per_run: 36000
    max_epochs_per_run: 500
  checkpoint_epochs: 2

  # Settings for HTCondor
  condor:
    bid: 25
    num_cpus: 4
    memory_cpus: 131072
    num_gpus: 1
    memory_gpus: 15000

  # Settings for Weights & Biases, remove if not used
  wandb:
    project: "fm4ar"
