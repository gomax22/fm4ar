# -----------------------------------------------------------------------------
# DATASET AND SCALERS
# -----------------------------------------------------------------------------

dataset:
  name: "inara_subset"
  data_dir: "$FM4AR_DATASETS_DIR/PSG_INARA_Subset_new"
  verbose: true
  limit: null
  random_seed: 42

theta_scaler:
  method: "MeanStdScaler"
  kwargs:
    dataset: "inara_subset"
    file_path: "$FM4AR_DATASETS_DIR/PSG_INARA_Subset_new/norm_params.pkl"

flux_scaler:
  method: "MeanStdScaler"
  kwargs:
    dataset: "inara_subset"
    file_path: "$FM4AR_DATASETS_DIR/PSG_INARA_Subset_new/norm_params.pkl"

error_bars_scaler:
  method: "MeanStdScaler"
  kwargs:
    dataset: "inara_subset"
    file_path: "$FM4AR_DATASETS_DIR/PSG_INARA_Subset_new/norm_params.pkl"

wlen_scaler:
  method: "UnitsScaler"
  kwargs:
    dataset: "inara_subset"
    scale: 1.0e-4
    offset: 0.0

# -----------------------------------------------------------------------------
# MODEL ARCHITECTURE
# -----------------------------------------------------------------------------

model:

  # General settings
  model_type: "npe"
  random_seed: 42

  # Embedding network for context
  context_embedding_net:
    # - block_type: "SoftClipFlux"
    #   kwargs:
    #     bound: 100.0
    - block_type: "Unsqueeze"
      kwargs:
        keys: ["flux", "error_bars", "wlen"]
        dim: 1
    - block_type: "Concatenate"
      kwargs:
        keys: ["flux", "error_bars", "wlen"]
        dim: 1
    - block_type: "HierarchicalMultiScaleSpectralEncoder"
      kwargs:

        # Transformer settings
        n_channels: 3
        d_model: 128
        n_heads: 4
        n_layers_per_scale: 2

        # Multi-scale settings
        patch_sizes: [2048, 4096, 8192, 16384]
        strides: [256, 512, 1024, 2048]

        # Transformer block settings
        mhsa_block_type: 'classic'
        mhca_block_type: 'classic'
        pre_rms: true
        post_rms: true
        use_qk_norm: true
        learn_qk_scale: false
        ffn_type: 'swiglu'
        ffn_hidden_mult: 4
        ffn_dropout: 0.1

        # Rotary Positional Encoding settings
        rope_type: 'none'

        # Token-positional encoding
        use_wavelength_token_embedding: true

        # Patch-positonal encoding
        max_len: 2048
        pe_type: 'wavelength'
        transform: 'log'
        scale_factor: 1.0
        base: 10000.0

        # Fusion settings
        incremental_cross_attention: true

  # Discrete normalizing flow (wrapped in a compatibility layer)
  # Note: normflows uses slightly different kwargs (see unit tests for demo)
  flow_wrapper:
    flow_library: "glasflow"
    kwargs:
      num_flow_steps: 16
      base_transform_type: "rq-coupling"
      base_transform_kwargs:
        hidden_dim: 1024
        num_transform_blocks: 4
        activation: "GELU"
        dropout_probability: 0.1
        use_batch_norm: false
        num_bins: 16
        tail_bound: 10.0


# -----------------------------------------------------------------------------
# TRAINING
# -----------------------------------------------------------------------------

training:

  stage_0:
    backup_interval: 10
    batch_size: 512
    n_workers: 16
    data_transforms:
      - type: "AddCustomNoise"
        kwargs:
          type: "DefaultNoiseGenerator"
          kwargs:
            sigma_min: 0.05
            sigma_max: 0.50
            random_seed: 42
    early_stopping:
      stage_patience: 100
    epochs: 600
    float32_matmul_precision: "high"
    gradient_clipping:
      enabled: true
      max_norm: 1.0
    logprob_evaluation:
      interval: 10
      n_samples: 2048
    optimizer:
      type: "AdamW"
      kwargs:
        lr: 5.0e-5
    scheduler:
      type: "CosineAnnealingLR"
      kwargs:
        T_max: 600
    use_amp: false

# -----------------------------------------------------------------------------
# LOCAL SETTINGS
# -----------------------------------------------------------------------------

local:

  # Device ("cpu" or "cuda"; or "auto")
  device: "cuda"

  # Settings for Weights & Biases, remove if not used
  wandb:
    project: "fm4ar"
    group: "inara-subset-new"
    name: "npe__0_standard-modeling-hierarchical-multi-scale-transformer"