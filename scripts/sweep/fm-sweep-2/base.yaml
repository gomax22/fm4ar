# -----------------------------------------------------------------------------
# DATASET
# -----------------------------------------------------------------------------

data:
  name: vasist-2023
  which: train
  file_name: selected.hdf
  train_fraction: 0.98
  add_noise_to_flux: True
  standardize_theta: True
  n_samples: 8_000_000


# -----------------------------------------------------------------------------
# MODEL ARCHITECTURE
# -----------------------------------------------------------------------------

model:

  # General settings
  model_type: fm
  t_theta_with_glu: True
  context_with_glu: False
  sigma_min: 0.0001
  time_prior_exponent: 2.0

  # Embedding network for context
  context_embedding_kwargs:
    01_SoftClip:
      model_type: SoftClip
      kwargs:
        bound: 100.0
    02_DropFeatures:
      model_type: DropFeatures
    03_DenseResidualNet:
      model_type: DenseResidualNet
      kwargs:
        hidden_dims: [2048, 1024]
        activation: elu
        output_dim: 1024
        dropout: 0.3
        batch_norm: True

  # Embedding network for t_theta
  t_theta_embedding_kwargs:
    01_PositionalEncoding:
      model_type: PositionalEncoding
      kwargs:
        n_freqs: 8
        encode_theta: False
    02_DenseResidualNet:
      model_type: DenseResidualNet
      kwargs:
        hidden_dims: [1024, 1024, 512, 512]
        activation: gelu
        output_dim: 512
        dropout: 0.0
        batch_norm: False

  # Posterior network (i.e., the network that predicts the vector field)
  posterior_kwargs:
    model_type: DenseResidualNet
    kwargs:
      hidden_dims: [
        4096,
        4096,
        2048,
        2048,
        2048,
        1024,
        1024,
        1024,
        512,
        512,
        512,
        256,
        256,
        256,
        128,
        128,
        128,
        64,
        64,
        64,
        32,
        32,
        32,
      ]
      activation: gelu
      dropout: 0.3
      batch_norm: True


# -----------------------------------------------------------------------------
# TRAINING
# -----------------------------------------------------------------------------

training:
  stage_0:
    epochs: 300
    optimizer:
      type: adamw
      lr: 1.0e-4
    scheduler:
      type: cosine
      T_max: 300
      eta_min: 1.0e-7
    batch_size: 8192
    early_stopping: 100
    gradient_clipping:
      max_norm: 1.0
    use_amp: True
    float32_matmul_precision: high
    logprob_epochs: 10


# -----------------------------------------------------------------------------
# LOCAL SETTINGS
# -----------------------------------------------------------------------------

local:

  # General settings: device, runtime limits, ...
  # Note: If training locally on a Mac, set num_workers to 0!
  device: cuda
  num_workers: 2
  runtime_limits:
    max_time_per_run: 14_400

  # Settings for HTCondor
  condor:
    bid: 25
    num_cpus: 2
    memory_cpus: 100_000
    num_gpus: 1
    memory_gpus: 20_000

  # Settings for Weights & Biases, remove if not used
  wandb:
    project: "fm4ar"
